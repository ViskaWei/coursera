{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "W2 Coding Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "osxVhElHKapV",
        "JXpmtF-uKapX",
        "SiC14wElKape",
        "NahVYLPKKapm",
        "pFCwsZzVKapy",
        "EjIP1ggBKap1",
        "xnv7LMUXKap6",
        "jGNAZCCYKap7",
        "yfnMhy6xKap9",
        "8x7I47G6KaqE",
        "ooeqV6GxKaqL",
        "XxhEIraGKaqb",
        "rsUT-G8YKaqd",
        "wtBXmPJIKaql"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ViskaWei/coursera/blob/master/W2_Coding_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIvqaAUzKaos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3624de9c-357c-4102-b335-090f47f8882c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "\n",
        "print('TF version:', tf.__version__)\n",
        "print('TFP version:', tfp.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.8.0\n",
            "TFP version: 0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9HBWGVKaox"
      },
      "source": [
        "# Probabilistic layers and Bayesian neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVYM_IToKaox"
      },
      "source": [
        "## Coding tutorials\n",
        "#### [1. The DistributionLambda layer](#coding_tutorial_1)\n",
        "#### [2. Probabilistic layers](#coding_tutorial_2)\n",
        "#### [3. The DenseVariational layer](#coding_tutorial_3)\n",
        "#### [4. Reparameterization layers](#coding_tutorial_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHdmVjG1Kaoy"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## The `DistributionLambda` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd8sUI97Kaoy"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGyQeDUaKao2"
      },
      "source": [
        "#### Create a probabilistic model using the `DistributionLambda` layer\n",
        "\n",
        "Create a model whose first layer represents:\n",
        "\n",
        "$$\n",
        "y = \\text{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7dgf5p8Kao2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a645e6ac-71de-4dd8-e4ec-787e1ed77e51"
      },
      "source": [
        "# Create a sigmoid model, first deterministic, then probabilistic\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(input_shape=(1,), units=1, activation='sigmoid',\n",
        "          kernel_initializer=tf.constant_initializer(1),\n",
        "          bias_initializer=tf.constant_initializer(0)),\n",
        "])\n",
        "\n",
        "# Plot the function\n",
        "x_plot = np.linspace(-5, 5, 100)\n",
        "plt.scatter(x_plot, model.predict(x_plot), alpha=0.4)\n",
        "plt.plot(x_plot, 1/(1 + np.exp(-x_plot)), color='r', alpha=0.8)\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXjV9Zn38fedc5Kc7CELAbKQsMnmAkZAQdwFl0oXq2JrsXWk7dQ+duo4j31mnk4vZzqdeWZq7WKr1mqrXVzQQepgxbpHpYIIskPCEgIkhJCcrCfJSb7PH8mhEYMEOMlJTj6v6/LqWX5yvsdefq7b+/5+f8ecc4iIyNAXE+kFiIhIeCjQRUSihAJdRCRKKNBFRKKEAl1EJEp4I/XBWVlZrrCwMFIfLyIyJL3//vuHnXPZvb0XsUAvLCxk7dq1kfp4EZEhycz2Hu89tVxERKKEAl1EJEoo0EVEooQCXUQkSpww0M3sUTM7ZGabjvO+mdlPzKzUzD40s5nhX6aIiJxIX3a5/Br4GfD4cd6/CpjY/dds4Bfd/ysiMuztrm6kpKyGKn8ATwwYEOyEnDQf88ZnUpSdHLbPOmGgO+feNLPCT7hkEfC467pt42ozSzez0c65g2Fao4hIxBwvkI/3OCfNR+GIBPbUtrDtQD3ltc1MGZVCYpyH1buO4IALxmfQFAjy1NoKbizOC1uoh2Mfei6wr8fziu7XPhboZrYUWApQUFAQho8WETl1Jwrr6oa2XgN5yqhktlU2fvSxc1yYn0RNWTVvlVVSnBXHiJo6Uhqa8H4YIDYYYEGwjdj2NsobZpJ94XkAlJTVDKpA7zPn3MPAwwDFxcW6EbuI9JvTCevtB+pJbG5gZHsDExobid3UQHyggWsDTSQ2NxCsreMi10pCSxPU13NjRxu+QBMe1wnAok6HJ8bo6P5f56A12IEv1gPAioQUuPA8kn1eKv2BsH3ncAT6fiC/x/O87tdERPpdb8H9SWG9q/wwaf4aRrXUcp6/lpT6I6Q31nJWo5/khjpi62pIb2smptMdDeGjgRznJeBLpNLiic/MIJCcyt64dNJzsmhNSKQ2Jp5WXwKe5CRqLY7YlGQavfFYUiIba4OMGZ1BhzeO2Hgvc4DGQJCcNF/Y/lmEI9BXAHeY2ZN0DUP96p+LSDj1DO0T9qidI7ujicLDVSSsryTTf4gldYcZcaSKhOpK0ttagL9WzJ0Yh2OTcCOzaczIojQtl7TcUTSlpLE14CU1N4eO9HQ+qHeMysuBmBj2+5sZk5aIwUcex8XGgIP6lnZSE2OZkJXEpr21xJiRlug43NaJCwa5IDeV+pZ26lraWTAtJ2z/nE4Y6Gb2B+BiIMvMKoB/BmIBnHMPAiuBq4FSoBn4cthWJyLDVijEe4Z2QWYS5YebePb9CmYVjqC95ggTyncz4i/7yKk5QPHhA2RVHyCmof5oZd3cCe3ZOdRljuTD5HNIHpuPf0QWG9p9pBTmE8zIYFNVM+Oykz8W0G0dHVR2OGLMGJHsqAsEccC5+elHe+g9H3cNOzsoP9LMpJxkMpLjmZyTwpbKBooyE8lIjsOA9g7ISPayYFpOWHe5WKR+U7S4uNjp5lwi0lNvIX7QH6AhECShwc9lrprgpi2MKC8j/1A5CXVHjvaoa2ITCOSNpWZkLhu9I/CMK6Q5ZzSrW+IpzEnrNayD3WFt5mhp6/zYwDMU0McGcl93uYT+iyKc2xPN7H3nXHFv70XsbosiItB7iDc0tzKqej8pa7czee92Jh7aQ2rNIcwg6MCfk8vOgsn45xaxPzuf+ryxrK23o1V2z7Aek+jwN7cdt5ruLaxnj8s4WknnZyWxeHbBSQfyheH/R3VCCnQRGXAfC/GcZDwV5cze9iFFu7Zw3Z4tpLa14BwcTkjlQNEk1s++jJ3ZhTQVjqem03u0R721u0edN+Kvwd3fYT1YKdBFZEAcG+LTM+PJ3LCOs7Z8wMSdG0irPUSsJ4a61Ew2jDubuslnUjVhKtssGV+clxgzUhK8jE7xUba39oQ96mgL675QoItIvzk2xM9KiyHvvdXM3fgeE0o/xNPWivkS2FU0lZfOvQL/mTNpyRnNQX8AX6yHGDNyukN8S2UD6b5Y8rOSuHBiFntqW6j0B4ZlcB+PAl1Ewqq3Snz02neZt+5tJpR+SEx7G21pI9g840Lezp9G8/QZxCbEc7CupSvEW7r2Zh8b4seGdiR61IOdAl1ETltvPfGkHVtYtOYNpm9aTWygmba0EXx43qW8XngO7ZOnER/v7QrxTiOmub1PIS6fTIEuIqekt90pgbp65qx9g+K1r5J1qALz+dgydRavjj+X5jNnEh/vpaquBV+wk0CHQjzcFOgictJ2Vzfy1NoK0hNiqQ+0k32kivEvPsFnNrxFUkc7B8YU8eSVt1A5+yJikpO6KnGFeL9ToItIn4Wq8j9vqSQ+1sNFgQNc/exTTNu5nqB5eHdSMTvnX0Vt4YSuEDeP2ikDSIEuIn1ytCr3eSncs41LS1ZQsGsLgYQk3pp/HZvmLqAsGNc12FSIR4QCXUQ+Uc+qfPzBMua98zypWzfSkJTGq1d9kfdmXEiLJ75ri2Hax7cYKsQHjgJdRD7m2IHn+a6WxU8/zpSd66lPSuW9z32ZlePn4E1KoLW9g8kjUxTig4ACXUQ+oufAs73mCNf96SlmrnuDYHw8r192PWvPX4AlJXJOVhKbDtQDphAfJBToIvKR+43vqWliTEoc099+lfm/fZSktgBrZ1/Oq/MX0RCfhC/OQ2tzG3HeVMZlJ4f1NzHl9CjQRYa5nhX5qDQflX/5gPkv/oaJR/axo2Aqv1v4BRpzC2hqaac4P/1oVZ7kC//9vOX0KNBFhqljtyCenRXPlOXPcOEfn6M+IZlnrr+DzosvYV95HTHN7aQkeInzelSVD2IKdJFhqGdVbhh5e3cy+0cPk9dUTdlFV/LYudfSGJfANSm+o3czTPfFqiof5BToIsNQSVkN6QmxpHsd1775LOe+9QINqZk8vuQ75F52IUXVjRyoD+huhkOMAl1kGAm1WZZ/UMG0YB0XP/8QI8p38dZZ83j9mltoiIklpaWdGE8Md10xSSE+xCjQRYaJnm2Wy3ev49Llj9Lh8VDytXs4cNYs2g7UQ7BTbZUhTIEuEuV6Dj8TrZP5Jc8w/q1VbM0Zx3Of/TpuVA7TNOyMCgp0kSjWsypPra/l5mU/Y1RFGVs/9Xk2XrsYV9PCwboAs4oyVZVHAQW6SBQLDT8L9u3g4kfvxdPWyvLFd7L77NnMSU9iWnwcs8ZlcsucsZFeqoSBAl0kCvUcfl6++31mvPAYTRnZ/OLau2nILaC1uY36lnbqWtpZMC0n0suVMFGgi0SZo22WeA83rHmBWa8uZ2vhFHZ8+58Ym5isk55RTIEuEmVKymrIiIULfvtTsktepeScC1l59a0kNsG0VA0/o5kCXSRKhNosK/9SytdW/pLsPZvYfcMS9l90HYnVTRp+DgMKdJEoEGqzjOxo4VvP/ojsit384epbiblkEVnJPuJivRp+DgMKdJEoUFJWQ05bI5f+9HvE1Rzg0evvYOfkc0k51Eicx6Ph5zChQBcZwkJtllff3Mi3lt1HbLOfDXffS0reJFIONarNMswo0EWGqFCbZXRLHX/39H8RX+/n54vvYmT+pK42i9ejNsswE9OXi8xsoZltN7NSM7unl/cLzOw1M/vAzD40s6vDv1QR6amkrIZRgXouu/+7ZLS38Osv3k3F2EnsPNR4dI/5vPGZkV6mDKATBrqZeYAHgKuAqcBiM5t6zGX/BDztnJsB3AT8PNwLFZEuu6sbeWL1Xv781mZm/OAerK6WDfd8n9EXFJPi83KwLkCSz6uticNQX1ous4BS59wuADN7ElgEbOlxjQNSux+nAQfCuUgR6dJzN8udz/6IhNoafnHz3YwcWaA2i/Qp0HOBfT2eVwCzj7nme8AqM/smkARc3tsfZGZLgaUABQUFJ7tWkWGvpKyGLNq55Gf34muo4cEbv0XF2En4tZtF6GMPvQ8WA792zuUBVwNPmNnH/mzn3MPOuWLnXHF2dnaYPlok+oXaLC+s2c0Z991Lwp4yNv2vfyTz4rlqs8hRfanQ9wP5PZ7ndb/W023AQgDn3Ltm5gOygEPhWKTIcNbz3iy3vfwb8nZu4reLlhI34Uy1WeQj+lKhrwEmmlmRmcXRNfRcccw15cBlAGY2BfAB1eFcqMhwVVJWQ7rPy+znHuPsbWt46fIb2XT2XO1mkY85YaA754LAHcBLwFa6drNsNrN7zey67svuAm43sw3AH4BbnXOuvxYtMpxU+QNMf3MlBa+8QMU1n6PtppvVZpFe9elgkXNuJbDymNe+2+PxFmBueJcmMryFToG2v/IKecseZM95F7D9hi+TFROjNov0KlxDUREJo1DfPG7bVpb8zyPszSnivgtvobqpVW0WOS4FusggFDoFOv/BH9CZkcm2u/+ZpNQk1pX71WaR49K9XEQGkVCb5YX3dnHXMz+ks7mZdf/8r6TkjmK+c1T6A2qzyHGpQhcZJEJtlqaWdr782u/I3lfGrxfexp70roNCjYEgOWm+CK9SBjNV6CKDRElZDekJsZz92grGbV7NHy/+LDsnz9Q9zaXPVKGLDBJV/gBjyzYxadlvODxnPs1f+JK2J8pJUYUuEmGhvvneLWVc88j3OZI1mk233UmWL0E/HScnRRW6SASF+uYtjS18/cVfQlsbP77ydirb0fZEOWkKdJEICvXNz1v+ODn7yth6+9/Rll+g7YlyStRyEYmgKn+Ac7avoeCVF9iz8DMELrpE2xPllKlCF4mgotZapv7qJ9SNn8yO65cA2p4op04VukgE7K5u5J1tB5n6w+9T19rByzd+g0yPh8buvrm2J8qpUIUuMsBCg9CiPzxKwaG9rFvyTda0+th+sF59czktqtBFBlhJWQ2Td67njNf+h/LLryXm0kuZ19JOks+rvrmcFlXoIgPMv+8gs3/7AA35RWy/8SsAJPu8VPkDEV6ZDHWq0EUGyO7qRkpKDzPpoftorvXz1je/R1xsHKBBqISHKnSRARDqm4968XnOLt/CHy++gRebE6luaNEBIgkbBbrIACgpqyG/9iDnLH+c2pmzibnh86QlxOoAkYSVWi4iA6C6poHP/ObHBH2JbPrKnWSlJjA/xacDRBJWCnSRfhK66VaVP0Des7/Dt6uUzd/+Lm1p6YD65hJ+armI9IOjP1YRCDK5eg/zS17g9Umzea9gOp3OqW8u/UIVukg/CN10Kz2mg7Mf+RFu5Egqbv0aB/wB4rwectJ8LJiWo765hJUCXaQfVPkDjErzMen3j5FYdYA1//vfGJ0/EvMH+PsFZ0R6eRKl1HIR6Qc5aT7iN25g7J//SPll13JkylnqmUu/U6CLhNHu6kaeWL2X0j3VFP3iPmpSMth2/ZfUM5cBoUAXCZOeg9Dr3n2e3IbDPHHFErbUtmuvuQwI9dBFwiQ0CC2o2Enhyysov/JT5F5+oW66JQNGFbpImFT5A6R6Opn+6E8IZGSz44ZbddMtGVAKdJEwyUnzkf/s70k6WMGmL99Bhy9Bg1AZUGq5iJym0InQ6rUfcsGKp9ky52Kqp83Qrw/JgFOFLnIaQoPQ5qZWPv0/j2Fp6Tx+/uf060MSEX0KdDNbaGbbzazUzO45zjU3mNkWM9tsZr8P7zJFBqfQIPTMt1aStqeU3bd9g3PPHMuk0ancMmeswlwG1AlbLmbmAR4ArgAqgDVmtsI5t6XHNROB7wBznXO1ZjayvxYsMphU+QMUtfmZ+NxvqT5nFlXFc0kGKjUIlQjoS4U+Cyh1zu1yzrUBTwKLjrnmduAB51wtgHPuUHiXKTI45aTGM+mxn+E8HjZ/6W/BTINQiZi+BHousK/H84ru13qaBEwys7fNbLWZLeztDzKzpWa21szWVldXn9qKRQaB0InQjpUvkrRuDW8vuJGWEZk6ESoRFa6hqBeYCFwMLAZ+aWbpx17knHvYOVfsnCvOzs4O00eLDKzQILT9cC0LXvo9jePP4Llx52sQKhHXl22L+4H8Hs/zul/rqQL4i3OuHdhtZjvoCvg1YVmlyCASGoQW//6XxDY1svfuf2Vu9kidCJWI60uFvgaYaGZFZhYH3ASsOOaa5XRV55hZFl0tmF1hXKfIoFHlD1Cwdxu5b73MnoWfpqGgSCdCZVA4YYXunAua2R3AS4AHeNQ5t9nM7gXWOudWdL93pZltATqAu51zNf25cJGBFjpAtGXvYS555Ef4R2RRumgxoJ+Tk8GhTydFnXMrgZXHvPbdHo8d8O3uv0SiTqhvnp4Qy03b3yC9aj+P3fh3ZLU54jt1IlQGB50UFemDUN98VH01M15+ltrzL6Rq+kzWlfs1CJVBQ/dyEemDKn+AUanxTH3iFziPl723fp356RlU+gMahMqgoQpdpA9y0nykv/0GmZs+YMfnvkTriEz1zWXQUYUu0gcXjvLR+eQj1OQVsefSq3UnRRmUFOginyC0syX/Vw8wuaGOVV+7h0MNbeSk+VgwLUd9cxlUFOgixxHa2VJYXc70t1dRetFCDuSO58aZuQpyGZTUQxc5jpKyGtLjPcx66mHaU1Ipv+nLpCfEUlKmIxYyOCnQRY6jyh9gynuvkrZrB9tv+grBpGSdCJVBTYEuchz5BJjw9K+pPWM6B86/BNCJUBncFOgixwjdGjfrsYcI1NWz6lNL6ATdGlcGPQW6SA+hQahv4wbO2VBC6ZWf5h2XrlvjypCgXS4iPZSU1TAizpj11C8JZGZTt3gJ85xHt8aVIUEVukgPVf4A00r+RPL+vWy9eSkdPp8GoTJkKNBFehjb0ci4535H9dnncWjmHECDUBk61HIR4a8nQgse+QVNTQFWXfVFMkBH/GVIUYUuw15oEJq0bg3Tt77Hjquu5732RA1CZchRhS7DXklZDRlex3lPP0JzzhgabryZeUHTIFSGHFXoMuxV+QOc+dofSaw6wJZbvkZnbJwGoTIkKdBl2CtsraNoxZNUnjePmukzAQ1CZWhSy0WGrd3VjZSUHmb8Qz+hrs3x8oLFZDlHYyCoQagMSarQZVgKDULTVpdwxq6NbL9uMWtb4jQIlSFNFboMSyVlNWRZkHOXPUpjXiHNn7meee1Og1AZ0lShy7BU5Q9w9qpl+I4cZvOSv8V5vRqEypCnQJdhaXxDFQUvLmf//CuomzgV0CBUhj61XGRY2V3dSMnOaqY8eD+HieXly24gR4NQiRKq0GXYCA1Cs15bRdH+UrZcv4QP6tEgVKKGKnQZNkrKahjZ0cKM5Y9TO3Eq7Vdfy7zWDg1CJWqoQpdho8of4Nznn8Db3MyWJd+AmBgNQiWqKNBl2JhSWcroN19mz1WfoTGvqyLXIFSiiVouEvV2VzfyzraDnP3g/VQkpPPK3E+Rq0GoRCFV6BLVQoPQ3BXLGF1XxcYvfI2NNW0ahEpU6lOgm9lCM9tuZqVmds8nXPc5M3NmVhy+JYqcupKyGvLqq5n20jIqz5tH7PwLmTchi0mjU7llzliFuUSVEwa6mXmAB4CrgKnAYjOb2st1KcCdwF/CvUiRU1VV18KsJx+k0xvHtptvB9AgVKJWXyr0WUCpc26Xc64NeBJY1Mt1/wL8B6B/UyTidlc38sTqvXhWvkDc+g9Ye90XaB2RCWgQKtGrL4GeC+zr8byi+7WjzGwmkO+c+59P+oPMbKmZrTWztdXV1Se9WJG+CPXNg4cOc/2by9g9ZjxP5MyguqGF+u7fCJ03PjPSyxQJu9MeippZDHAfcNeJrnXOPeycK3bOFWdnZ5/uR4v0qqSshvSEWM5b/hsSgm3s/dq3SE2KZ125X4NQiWp92ba4H8jv8Tyv+7WQFGA68LqZAYwCVpjZdc65teFaqEhfVfkDTN+7mdHvvk7ZdTcRN2ki852j0h/QiVCJan2p0NcAE82syMzigJuAFaE3nXN+51yWc67QOVcIrAYU5hIxY+IdUx79KU1j8tn1qRsA9c1leDhhoDvngsAdwEvAVuBp59xmM7vXzK7r7wWK9FVoEJrx6EN0VFXx0mf+hqA3Vn1zGTb6dFLUObcSWHnMa989zrUXn/6yRE5OaBA6fv9Ozlv7KusvuYY3fGMoOljPpNGpLJiWo765RD0d/ZeoUFJWQ4bXMft3PyeQkc3hJX/DPOfVnRRlWNHRf4kKVf4AM/70NEmV+9n05Tvo8CXoAJEMOwp0iQqTasoZu/I59s+/gprpMwENQmX4UctFhqzd1Y2UlNVQfbieax+6j9qEVN779BJ8upOiDFOq0GVICg1BmwJB5r2+nPTKCl753O00euOp9Ad0gEiGJVXoMiSFToPmH9zN+BefZf9FVxIzdy6ZGoLKMKYKXYakKn+AVAty5sM/JJCewbabbtMQVIY9VegypIT65hv31zH25d8Tf2Af6//3vxFMSqaxpV1DUBnWVKHLkNGzb76wqZyZ76xi1fRL2FZwhk6DiqBAlyEk1DfP6Gxl/pO/wBUU8O5VN+kuiiLd1HKRIaPKH2BUmo+pD92Pr+4I6//pv7igKE93URTppgpdhoycNB8j3niF0avfoHTRYvzjJunwkEgPqtBl0AsNQvdvKuW8X/2UfUWT2HntDTR29811eEikiyp0GdRCg9DmplauX/4QiXFeHrniVrZXNapvLnIMVegyqIUGoTP+9DQjyrbx4Vf/nunnTNFdFEV6oUCXQa3KH2Da/u2MX/EkB+ZeysHzLya5++fkROSjFOgyKIX65ru27+WqX/6A2qzRbLnl64DuoihyPOqhy6Bz9ABRcxtLX/kNsc2N/PTyr1DZjg4QiXwCBboMOqG++TmvLid/x0Z2fvGrtBSO0wEikRNQy0UGnSp/gGkV25j437+jcvZ8/Fd9ivmgA0QiJ6BAl0Ej1Dffu6WMax75PkeyRrPpy98EM914S6QP1HKRQSHUN29paObrKx+GtjbuX3C7+uYiJ0GBLoNCqG8+67nHyKnYxdalf0d7XoH65iInQS0XiahQm2X5BxVcvf1tRr+6kt3XfJ7A/EuY373fXH1zkb5RhS4R0/P+5rOO7OHiFY+zPn8a71x1I6D95iInSxW6REyozZLTeISrn32AqhHZPPfZrxN3uJm42FjdeEvkJKlCl4ip8gdIc23MvP9e4nFs+vZ3iUtP5WBdQH1zkVOgCl0GXKhvvrn8MOc+/WPi95ez/u5/IX7CeKa1tDNrXKb65iKnQIEuAyrUN0/3efmbd5cxasdGnrh6CXEFZxCv+5uLnBa1XGRAhfrmZ7/+Rya/+wr7PvV5dp5/ubYnioSBKnQZED23J16xey3jnv8VledfzIEvfIX5ZtqeKBIGfarQzWyhmW03s1Izu6eX979tZlvM7EMze8XM9G+mHNVze+Lcyu1cuexhNo2ZzGs3fQ1iYrQ9USRMThjoZuYBHgCuAqYCi81s6jGXfQAUO+fOApYB/y/cC5WhK9RmKago5aZlP+PgyHyeuuGbbK9t07F+kTDqS4U+Cyh1zu1yzrUBTwKLel7gnHvNOdfc/XQ1kBfeZcpQVuUPMKZqL+f+6Ht0ZGWz6R/uJT4tRdsTRcKsLz30XGBfj+cVwOxPuP424MXe3jCzpcBSgIKCgj4uUYaqUN+8cv1mrnn8BzQnJ7H+7n8lJTtH2xNF+kFYd7mY2ReBYuA/e3vfOfewc67YOVecnZ0dzo+WQSbUN7e95Xzz6R/Siof//PSdlPtS1WYR6Sd9CfT9QH6P53ndr32EmV0O/CNwnXOuNTzLk6GqpKyGPP8hLr3//xLniWH9Pd8nmJuv7Yki/agvLZc1wEQzK6IryG8Cbu55gZnNAB4CFjrnDoV9lTJkhNos7778F775zH0EYz2s/84PiMsv1N0TRfrZCSt051wQuAN4CdgKPO2c22xm95rZdd2X/SeQDDxjZuvNbEW/rVgGrVCbxbtzJ3c+9UOCzrj/83ezZ8QoQHdPFOlvfTpY5JxbCaw85rXv9nh8eZjXJUNIqCr/85ZKxh3cxbxlP6Yj0ceD13+b2qzRtB9qJM7j0bF+kX6mk6JyWo7emyUhlkk7N3DjsgeoShnBhrv+haLMHIKHGjlYF2BWUSYLpuWoby7SjxToclpCh4Ymv/8mRU//hIOjCnj6i39P0CUzJ8VHnNej7YkiA0SBLqfk6L1Z1u3j+nV/Yspby6madhaPLPwqMclJtDb/9RSo2iwiA0OBLict1GbJ8DqWrnqMSevf4e2zL+TA397JWZ5YNh2oB4wkn1dtFpEBpECXPus5/MwINPCpFb8ga9d2nr/0et6Zey0pR1qZNtrHuOxk7TMXiQAFuvRJz+Fn/r5SFj/7M2Jbmnl76d0EZpxPioafIhGnQJc+KSmrId3nZdq7q1j4259Tl5rJU7f8A3X5hRp+igwSCnT5RKE2y5/e3cGS1//ApG3vcXD6TH5+2a2Qlqrhp8ggokCX4wq1WYoqd3PPb/+dpNrDrLjos7Tc/AWmW4yGnyKDjAJdPiJUkVf5A+ytrmfhmj8x+8/P0ZCSziNf+g77CyaRcriZaaPTNPwUGWQU6HJUz8Hn+OZqZj/wfXIP7mH3BRex+2++SU5nLPUafooMWgp0+ch2xIQYx+e2vM45f3qGI50ell3/DSqK5zInKYUs0PBTZBBToA9zH9mOWFHGopW/JqtyH+Wz5/L+jbezsRaCDQE6naMxENTwU2QQU6APUz2r8hGtjcxf8wJXvbGK+uR0nv3Ctyk/s5g5uZlMiWvkQH2ASn+AnDSf2iwig5gCfRgJhfi2A/WU1zYzLcvH+atXcclby/G2trLtkmt45txriEn5671YYjwx3HXFJIW4yBCgQB8merZWGppbOWfTai55/Vky6qrZNfFsXln4BRrG5HFWVpK2I4oMUQr0KNeztRLvjeGKIztZ/ORvyD20j6qcfH5/87fZVnQmvngvrc1txHlTtR1RZIhSoEehY1srU3KSmbz9Ay5963lGHthD3Yhsnv/sVymbOY+61g6K89NVlYtEAQV6lPhYiI9KoamxmXM3vMX5775EzuEK6jJyePEzt7Nu+hza8BAT6CAlwUuc16OqXCQKKNCjQM/+eH2gndSmegQyGlsAAAj7SURBVEY+80euWfMyI1oaqMrJ578/vZT1U2cT74ujvb2DySNT2FLZQLovVlW5SJRQoA9hPfvjPo9xSdM+rlz5R87c9j7W0cGHBVNYecl1VJ5xJv5AkHN7tFbys5JYPLtAIS4SRRToQ8yxrZXzYlu4+I1VzPywhNTaagIJibw3+wq2nH85W7zp+GI9xLQE1VoRGQYU6INUz5tkeWLAgOqGNsprm5mZ2MGEd95kwQdvk19RihnsGTedNy+7no1nzCAQE0eMGTkJXkan+NRaERkmFOiDSG+DzcQ4D6t3HWFE7SFm7PqABVvXkVdRiuvspC63kDevuIENU2dxODULX5yH1mP642qtiAwfCvQI6y3E6wPtxAfbaXvnLxTt+pB/KN1A5uFKWoMdNBSM451LPsPrBWfhGT8BX2wM/pb2j2w9VIiLDE8K9AHQW/sk2AmeGDjU0MrYjCSaGpsprNjJyJJtFO/azMSDu4hpb6fRxVBzxnQ2zrmCFzMmkjahCF9sDPV1LfjagrS2m/rjIgIo0MPqk/rePdsnDrg022hYu4GJu3Ywq6GCy7ZvJb4ziHOwJ2MMa4sv5cCkM1mVkEfemEwMSOzooKU7xHPSfOqPi8hHKNBPwckEd1KckeE/gtuxnrRD5dxWuZfRB3aT2lBLR6cDj4fq3EJ2zrmCXfmTqB4/mV3B2K7dKWaMNIe/uQ0HXDA+g6ZAh/rjItIrBfoxPqk98knBnRzryKqtIVi6kYQjB1lcc4CsQ/tJPlDOiM42nIPWYActo/PYP24Kb2cXUDv+DMqz82k2LzPz09mwt7Zrd0raX3enFGUmkpEchwHtHSjEReS4ojLQTxTKx3t8vLCeMiqZbQcbSGxuYHSglum1R0hdfZhMfzVL66oZceQQvupKkjwcDW6XPoKakWNYc8YsguMn4M8dy1sundzcLAyIi41hQlYSR8pqSE2MISM5nsk52p0iIqfOnHMR+eDi4mK3du3ak/p7+hrUoUFjoD340VCubPzY46nZCZTvqSKpuZ7stgYSG+pJbKwnramO9CY/KfW1xNZUk91SjzfYTmuwA1+sB+fAHxNL+6gx1GWMZKs3HSscS+OoXFa3JzFy7GgMaOvoINjhiDHDzNHS1nm0fRLv9bKnpolRqfEEOyEnzce88ZkKcRE5LjN73zlX3Nt7farQzWwh8GPAAzzinPv3Y96PBx4HzgVqgBudc3tOZ9HH6nm/Em8Mfw3qnCRKK2qJawswd7SPPeXVOH8DWdlxHD5Yw2VNjSS0NNNaW0sx7SS0NNLp97OovYWElkY8jQ3ExRjAR8K60cXQmZVFY+oIdmblcyAvl4a0DDZ2JJBYmE8gayQf1HYwbmTKx4I7/Th972PbJxnJXr46f5wCXETC4oSBbmYe4AHgCqACWGNmK5xzW3pcdhtQ65ybYGY3Af8B3BjOhZaU1ZCeEMuUNa+R9swfuKw1QHxbAJqbie8OZDPo6AxVw12PPd3v1eLFk55Oa0IS5cRBbiEtSSlsb/WQlZdDS1IqGwNe0vNH05GezgdHgkfDer+/mTFpiUeD+1CHIwYjL6Nvwa32iYgMhL5U6LOAUufcLgAzexJYBPQM9EXA97ofLwN+ZmbmwtjPqfIHGJXmoy0ljX0j84lJSKQ93seupg5ycjJoi/dRZ7F4UlNp8MTR6I0nNi0Nf2wCbb4EKhpaj4Zyz4A+Nqz3dzhinJGXEXs0rM/NTz/aolFwi8hg1ZdAzwX29XheAcw+3jXOuaCZ+YFM4HDPi8xsKbAUoKCg4KQWmpPmozEQJGbGbFanTaA12PmxQA4NGjeW1ZCaGMvUUSns2HUEF+j4SCgf7/HxwjrYCbPHZSi4RWRQG9BdLs65h4GHoWsoejJ/77zxmTy1tgKAcVmJR3voxwZynNdDYVbS0UFjKIj78lhhLSJDWV8CfT+Q3+N5XvdrvV1TYWZeII2u4WjYFHUfay8pq6GxNXjcQNagUUSGq74E+hpgopkV0RXcNwE3H3PNCmAJ8C5wPfBqOPvnIUXZyQpqEZHjOGGgd/fE7wBeomvb4qPOuc1mdi+w1jm3AvgV8ISZlQJH6Ap9EREZQH3qoTvnVgIrj3ntuz0eB4DPh3dpIiJyMmIivQAREQkPBbqISJRQoIuIRImI3ZzLzKqBvRH58NOTxTEHpoaB4fadh9v3BX3noWSscy67tzciFuhDlZmtPd6dzqLVcPvOw+37gr5ztFDLRUQkSijQRUSihAL95D0c6QVEwHD7zsPt+4K+c1RQD11EJEqoQhcRiRIKdBGRKKFAPw1mdpeZOTPLivRa+pOZ/aeZbTOzD83sv80sPdJr6i9mttDMtptZqZndE+n19Dczyzez18xsi5ltNrM7I72mgWJmHjP7wMxeiPRawkWBforMLB+4EiiP9FoGwMvAdOfcWcAO4DsRXk+/6PH7uVcBU4HFZjY1sqvqd0HgLufcVGAO8I1h8J1D7gS2RnoR4aRAP3U/Av4BiPqpsnNulXMu2P10NV0/chKNjv5+rnOuDQj9fm7Ucs4ddM6t637cQFfA5UZ2Vf3PzPKAa4BHIr2WcFKgnwIzWwTsd85tiPRaIuArwIuRXkQ/6e33c6M+3ELMrBCYAfwlsisZEPfTVZB1Rnoh4TSgvyk6lJjZn4FRvbz1j8D/oavdEjU+6fs6557vvuYf6fpP9N8N5Nqk/5lZMvAs8C3nXH2k19OfzOxa4JBz7n0zuzjS6wknBfpxOOcu7+11MzsTKAI2mBl0tR/Wmdks51zlAC4xrI73fUPM7FbgWuCy/vh5wUGiL7+fG3XMLJauMP+dc+65SK9nAMwFrjOzqwEfkGpmv3XOfTHC6zptOlh0msxsD1DsnBuKd23rEzNbCNwHXOScq470evpL9w+c7wAuoyvI1wA3O+c2R3Rh/ci6qpLfAEecc9+K9HoGWneF/vfOuWsjvZZwUA9d+uJnQArwspmtN7MHI72g/tA9+A39fu5W4OloDvNuc4FbgEu7/79d3125yhCkCl1EJEqoQhcRiRIKdBGRKKFAFxGJEgp0EZEooUAXEYkSCnQRkSihQBcRiRL/H8b4eh148MIDAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THWcaknbKao5"
      },
      "source": [
        "# Create a constant input for this model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-N5tHQZKao8"
      },
      "source": [
        "# Explore the feedforward object...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ugBgTsxGKao-"
      },
      "source": [
        "# ... and its behaviour under repeated calls\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6I0aMgpKapD"
      },
      "source": [
        "#### Use the forward model to create probabilistic training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSL5TMgVKapD"
      },
      "source": [
        "# Use the model to create 500 training points\n",
        "\n",
        "x_train = np.linspace(-5, 5, 500)[:, np.newaxis]\n",
        "y_train = model.predict(x_train)\n",
        "\n",
        "# Plot the data and the mean of the distribution\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.scatter(x_train, y_train, alpha=0.04, color='blue', label='samples')\n",
        "ax.plot(x_train, model(x_train).mean().numpy().flatten(), \n",
        "        color='red', alpha=0.8, label='mean')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abtwq7yCKapG"
      },
      "source": [
        "#### Create a new probabilistic model with the wrong weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZDJVsXmKapG"
      },
      "source": [
        "# Create a new version of the model, with the wrong weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yENtha1KapJ"
      },
      "source": [
        "#### Train the new model with the negative loglikelihood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VagGrOU7KapJ"
      },
      "source": [
        "# Define negative loglikelihood, which we will use for training\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zQaoho3KapL"
      },
      "source": [
        "# Compile untrained model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnFKk9G5KapO"
      },
      "source": [
        "# Train model, record weights after each epoch\n",
        "\n",
        "epochs = [0]\n",
        "training_weights = [model_untrained.weights[0].numpy()[0, 0]]\n",
        "training_bias = [model_untrained.weights[1].numpy()[0]]\n",
        "for epoch in range(100):\n",
        "    model_untrained.fit(x=x_train, y=y_train, epochs=1, verbose=False)\n",
        "    epochs.append(epoch)\n",
        "    training_weights.append(model_untrained.weights[0].numpy()[0, 0])\n",
        "    training_bias.append(model_untrained.weights[1].numpy()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQUK9RCYKapQ"
      },
      "source": [
        "# Plot the model weights as they train, converging to the correct values\n",
        "\n",
        "plt.plot(epochs, training_weights, label='weight')\n",
        "plt.plot(epochs, training_bias, label='bias')\n",
        "plt.axhline(y=1, label='true_weight', color='k', linestyle=':')\n",
        "plt.axhline(y=0, label='true_bias', color='k', linestyle='--')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3g6c9OKapS"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Probabilistic layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H07K-uaiKapT"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osxVhElHKapV"
      },
      "source": [
        "#### Create data\n",
        "\n",
        "The data you'll be working with is artifically created from the following equation:\n",
        "$$ y_i = x_i + \\frac{3}{10}\\epsilon_i$$\n",
        "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-CA1wLXKapV"
      },
      "source": [
        "# Create and plot 100 points of training data\n",
        "\n",
        "x_train = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
        "y_train = x_train + 0.3*np.random.randn(100)[:, np.newaxis]\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.4)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXpmtF-uKapX"
      },
      "source": [
        "#### Deterministic linear regression with MSE loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF5MF6OPKapY"
      },
      "source": [
        "# Create and train deterministic linear model using mean squared error loss\n",
        "\n",
        "# Create linear regression via Sequential model\n",
        "model = Sequential([\n",
        "    Dense(units=1, input_shape=(1,))\n",
        "])\n",
        "model.compile(loss=MeanSquaredError(), optimizer=RMSprop(learning_rate=0.005))\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, epochs=200, verbose=False)\n",
        "\n",
        "# Plot the data and model\n",
        "plt.scatter(x_train, y_train, alpha=0.4, label='data')\n",
        "plt.plot(x_train, model.predict(x_train), color='red', alpha=0.8, label='model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhKQ05n6Kapb"
      },
      "source": [
        "# Examine the model predictions\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiC14wElKape"
      },
      "source": [
        "#### Probabilistic linear regression with both user-defined and learned variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx8f6mmwKape"
      },
      "source": [
        "# Create probabilistic regression with normal distribution as final layer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2NRMucxKapg"
      },
      "source": [
        "# Train model using the negative loglikelihood\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSJlVYI1Kapi"
      },
      "source": [
        "# Examine the distribution created as a feedforward value\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXepFof7Kapk"
      },
      "source": [
        "# Plot the data and a sample from the model\n",
        "\n",
        "y_model = model(x_train)\n",
        "y_sample = y_model.sample()\n",
        "y_hat = y_model.mean()\n",
        "y_sd = y_model.stddev()\n",
        "y_hat_m2sd = y_hat - 2 * y_sd\n",
        "y_hat_p2sd = y_hat + 2 * y_sd\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
        "ax1.scatter(x_train, y_train, alpha=0.4, label='data')\n",
        "ax1.scatter(x_train, y_sample, alpha=0.4, color='red', label='model sample')\n",
        "ax1.legend()\n",
        "ax2.scatter(x_train, y_train, alpha=0.4, label='data')\n",
        "ax2.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
        "ax2.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
        "ax2.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
        "ax2.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NahVYLPKKapm"
      },
      "source": [
        "#### Probabilistic linear regression with nonlinear learned mean & variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj0z3rEtKapn"
      },
      "source": [
        "Let's change the data to being nonlinear:\n",
        "$$ y_i = x_i^3 + \\frac{1}{10}(2 + x_i)\\epsilon_i$$\n",
        "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOF25FdlKapn"
      },
      "source": [
        "# Create and plot 10000 data points\n",
        "\n",
        "x_train = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
        "y_train = np.power(x_train, 3) + 0.1*(2+x_train)*np.random.randn(1000)[:, np.newaxis]\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbrN-Dv7Kapp"
      },
      "source": [
        "# Create probabilistic regression: normal distribution with fixed variance\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(input_shape=(1,), units=8, activation='sigmoid'),\n",
        "    Dense(tfpl.IndependentNormal.params_size(event_shape=1)),\n",
        "    tfpl.IndependentNormal(event_shape=1)\n",
        "])\n",
        "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.01))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRn7haPjKapr"
      },
      "source": [
        "# Train model\n",
        "\n",
        "model.fit(x_train, y_train, epochs=200, verbose=False)\n",
        "model.evaluate(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWdceGQPKapu"
      },
      "source": [
        "# Plot the data and a sample from the model\n",
        "\n",
        "y_model = model(x_train)\n",
        "y_sample = y_model.sample()\n",
        "y_hat = y_model.mean()\n",
        "y_sd = y_model.stddev()\n",
        "y_hat_m2sd = y_hat - 2 * y_sd\n",
        "y_hat_p2sd = y_hat + 2 * y_sd\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
        "ax1.scatter(x_train, y_train, alpha=0.2, label='data')\n",
        "ax1.scatter(x_train, y_sample, alpha=0.2, color='red', label='model sample')\n",
        "ax1.legend()\n",
        "ax2.scatter(x_train, y_train, alpha=0.2, label='data')\n",
        "ax2.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
        "ax2.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
        "ax2.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
        "ax2.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk_oEVCmKapw"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## The `DenseVariational` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR8mg4KbKapw"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFCwsZzVKapy"
      },
      "source": [
        "#### Create linear data with Gaussian noise\n",
        "\n",
        "The data you'll be working with is the same as you used before:\n",
        "$$ y_i = x_i + \\frac{3}{10}\\epsilon_i$$\n",
        "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed. We'll be running a Bayesian linear regression on this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPbuqlB5Kapz"
      },
      "source": [
        "# Use the same data as before -- create and plot 100 data points\n",
        "\n",
        "x_train = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
        "y_train = x_train + 0.3*np.random.randn(100)[:, np.newaxis]\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjIP1ggBKap1"
      },
      "source": [
        "#### Create the prior and posterior distribution for model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDaiVcweKap1"
      },
      "source": [
        "# Define the prior weight distribution -- all N(0, 1) -- and not trainable\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GdB92laKap5"
      },
      "source": [
        "# Define variational posterior weight distribution -- multivariate Gaussian\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnv7LMUXKap6"
      },
      "source": [
        "#### Aside: analytical posterior\n",
        "\n",
        "In this tutorial, we're using a variational posterior because, in most settings, it's not possible to derive an analytical one. However, in this simple setting, it is possible. Specifically, running a Bayesian linear regression on $x_i$ and $y_i$ with $i=1, \\ldots, n$ and a unit Gaussian prior on both $\\alpha$ and $\\beta$:\n",
        "\n",
        "$$\n",
        "y_i = \\alpha + \\beta x_i + \\epsilon_i, \\quad \n",
        "\\epsilon_i \\sim N(0, \\sigma^2), \\quad \n",
        "\\alpha \\sim N(0, 1), \\quad \n",
        "\\beta \\sim N(0, 1)\n",
        "$$\n",
        "\n",
        "gives a multivariate Gaussian posterior on $\\alpha$ and $\\beta$:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{pmatrix}\n",
        "\\sim\n",
        "N(\\mathbf{\\mu}, \\mathbf{\\Sigma})\n",
        "$$\n",
        "where\n",
        "$$ \n",
        "\\mathbf{\\mu}\n",
        "= \n",
        "\\mathbf{\\Sigma} \n",
        "\\begin{pmatrix}\n",
        "\\hat{n} \\bar{y} \\\\\n",
        "\\hat{n} \\overline{xy}\n",
        "\\end{pmatrix},\n",
        "\\quad\n",
        "\\mathbf{\\Sigma} = \n",
        "\\frac{1}{(\\hat{n} + 1)(\\hat{n} \\overline{x^2} + 1) - \\hat{n}^2 \\bar{x}^2}\n",
        "\\begin{pmatrix}\n",
        "\\hat{n} \\overline{x^2} + 1 & -\\hat{n} \\bar{x} \\\\\n",
        "-\\hat{n} \\bar{x} & \\hat{n} + 1\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "In the above, $\\hat{n} = \\frac{n}{\\sigma^2}$ and $\\bar{t} = \\frac{1}{n}\\sum_{i=1}^n t_i$ for any $t$. In general, however, it's not possible to determine the analytical form for the posterior. For example, in models with a hidden layer with nonlinear activation function, the analytical posterior cannot be determined in general, and variational methods as below are useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGNAZCCYKap7"
      },
      "source": [
        "#### Create the model with `DenseVariational` layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ee6qHUTKap7"
      },
      "source": [
        "# Create linear regression model with weight uncertainty: weights are\n",
        "# distributed according to posterior (and, indirectly, prior) distribution\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfnMhy6xKap9"
      },
      "source": [
        "#### Train model and inspect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0864YLRGKap-"
      },
      "source": [
        "# Fit the model, just like a deterministic linear regression\n",
        "\n",
        "model.fit(x_train, y_train, epochs=500, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzCT1byTKaqA"
      },
      "source": [
        "# Check out the parameters of the prior and posterior distribution\n",
        "\n",
        "dummy_input = np.array([[0]])\n",
        "model_prior = model.layers[0]._prior(dummy_input)\n",
        "model_posterior = model.layers[0]._posterior(dummy_input)\n",
        "print('prior mean:           ', model_prior.mean().numpy())\n",
        "print('prior variance:       ', model_prior.variance().numpy())\n",
        "print('posterior mean:       ', model_posterior.mean().numpy())\n",
        "print('posterior covariance: ', model_posterior.covariance().numpy()[0])\n",
        "print('                      ', model_posterior.covariance().numpy()[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RWg-xARKaqC"
      },
      "source": [
        "# Plot an ensemble of linear regressions, with weights sampled from\n",
        "# the posterior distribution\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.4, label='data')\n",
        "for _ in range(10):\n",
        "    y_model = model(x_train)\n",
        "    if _ == 0:\n",
        "        plt.plot(x_train, y_model, color='red', alpha=0.8, label='model')\n",
        "    else:\n",
        "        plt.plot(x_train, y_model, color='red', alpha=0.8)        \n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x7I47G6KaqE"
      },
      "source": [
        "#### Explore the effect of sample size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B3Sqy4aKaqF"
      },
      "source": [
        "# Create two datasets, one with 1000 points, another with 100\n",
        "\n",
        "x_train_1000 = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
        "y_train_1000 = x_train_1000 + 0.3*np.random.randn(1000)[:, np.newaxis]\n",
        "\n",
        "x_train_100 = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
        "y_train_100 = x_train_100 + 0.3*np.random.randn(100)[:, np.newaxis]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
        "ax1.scatter(x_train_1000, y_train_1000, alpha=0.1)\n",
        "ax2.scatter(x_train_100, y_train_100, alpha=0.4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDpDiW14KaqH"
      },
      "source": [
        "# Train a model on each dataset\n",
        "\n",
        "model_1000 = Sequential([tfpl.DenseVariational(input_shape=(1,), \n",
        "                                               units=1,\n",
        "                                               make_prior_fn=prior, \n",
        "                                               make_posterior_fn=posterior,\n",
        "                                               kl_weight=1/1000)])\n",
        "\n",
        "model_100 = Sequential([tfpl.DenseVariational(input_shape=(1,), \n",
        "                                              units=1,\n",
        "                                              make_prior_fn=prior, \n",
        "                                              make_posterior_fn=posterior,\n",
        "                                              kl_weight=1/100)])\n",
        "\n",
        "model_1000.compile(loss=MeanSquaredError(), optimizer=RMSprop(learning_rate=0.005))\n",
        "model_100.compile(loss=MeanSquaredError(), optimizer=RMSprop(learning_rate=0.005))\n",
        "\n",
        "model_1000.fit(x_train_1000, y_train_1000, epochs=50, verbose=False)\n",
        "model_100.fit(x_train_100, y_train_100, epochs=500, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_q1wfCwKaqJ"
      },
      "source": [
        "# Plot an ensemble of linear regressions from each model\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
        "for _ in range(10):\n",
        "    y_model_1000 = model_1000(x_train_1000)\n",
        "    ax1.scatter(x_train_1000, y_train_1000, color='C0', alpha=0.02)\n",
        "    ax1.plot(x_train_1000, y_model_1000, color='red', alpha=0.8)\n",
        "    y_model_100 = model_100(x_train_100)\n",
        "    ax2.scatter(x_train_100, y_train_100, color='C0', alpha=0.05)\n",
        "    ax2.plot(x_train_100, y_model_100, color='red', alpha=0.8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooeqV6GxKaqL"
      },
      "source": [
        "#### Put it all together: nonlinear probabilistic regression with weight uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ-gVKXVKaqM"
      },
      "source": [
        "Let's change the data to being nonlinear:\n",
        "$$ y_i = x_i^3 + \\frac{1}{10}(2 + x_i)\\epsilon_i$$\n",
        "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNopZXPcKaqM"
      },
      "source": [
        "# Create and plot 1000 data points\n",
        "\n",
        "x_train = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
        "y_train = np.power(x_train, 3) + 0.1*(2+x_train)*np.random.randn(1000)[:, np.newaxis]\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF9yxmDiKaqO"
      },
      "source": [
        "# Create probabilistic regression with one hidden layer, weight uncertainty\n",
        "\n",
        "model = Sequential([\n",
        "    tfpl.DenseVariational(units=8,\n",
        "                          input_shape=(1,),\n",
        "                          make_prior_fn=prior,\n",
        "                          make_posterior_fn=posterior,\n",
        "                          kl_weight=1/x_train.shape[0],\n",
        "                          activation='sigmoid'),\n",
        "    tfpl.DenseVariational(units=tfpl.IndependentNormal.params_size(1),\n",
        "                          make_prior_fn=prior,\n",
        "                          make_posterior_fn=posterior,\n",
        "                          kl_weight=1/x_train.shape[0]),\n",
        "    tfpl.IndependentNormal(1)\n",
        "])\n",
        "\n",
        "def nll(y_true, y_pred):\n",
        "    return -y_pred.log_prob(y_true)\n",
        "\n",
        "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.005))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGlYg-jnKaqP"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100, verbose=False)\n",
        "model.evaluate(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdeHFDzXKaqR"
      },
      "source": [
        "# Plot an ensemble of trained probabilistic regressions\n",
        "\n",
        "plt.scatter(x_train, y_train, marker='.', alpha=0.2, label='data')\n",
        "for _ in range(5):\n",
        "    y_model = model(x_train)\n",
        "    y_hat = y_model.mean()\n",
        "    y_hat_m2sd = y_hat - 2 * y_model.stddev()\n",
        "    y_hat_p2sd = y_hat + 2 * y_model.stddev()\n",
        "    if _ == 0:\n",
        "        plt.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
        "        plt.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
        "        plt.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
        "    else:\n",
        "        plt.plot(x_train, y_hat, color='red', alpha=0.8)\n",
        "        plt.plot(x_train, y_hat_m2sd, color='green', alpha=0.8)\n",
        "        plt.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)        \n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkinPW72KaqU"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## Reparameterization layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKiqoElQKaqV"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVagTkYgKaqX"
      },
      "source": [
        "#### Load in the HAR dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icl9TqZAKaqY"
      },
      "source": [
        "You'll be working with the [Human Activity Recognition (HAR) Using Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) dataset. It consists of the readings from an accelerometer (which measures acceleration) carried by a human doing different activities. The six activities are walking horizontally, walking upstairs, walking downstairs, sitting, standing and laying down. The accelerometer is inside a smartphone, and, every 0.02 seconds (50 times per second), it takes six readings: linear and gyroscopic acceleration in the x, y and z directions. See [this link](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) for details and download. If you use it in your own research, please cite the following paper:\n",
        "\n",
        "- Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. \n",
        "\n",
        "The goal is to use the accelerometer data to predict the activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLpIPEaEKe25"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset required for this coding tutorial can be downloaded from the following link:\n",
        "\n",
        "https://drive.google.com/file/d/1U_O3bhvuSAzQDKHGWcBIAO80iV9FVmDo/view?usp=sharing\n",
        "\n",
        "You should store this file in Drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15FeLLY8KfBU"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_p1wpaTKaqY"
      },
      "source": [
        "# Load the HAR dataset and create some data processing functions\n",
        "\n",
        "# Function to load the data from file\n",
        "def load_HAR_data():\n",
        "    data_dir = '/path/to/HAR/'\n",
        "    x_train = np.load(os.path.join(data_dir, 'x_train.npy'))[..., :6]\n",
        "    y_train = np.load(os.path.join(data_dir, 'y_train.npy')) - 1\n",
        "    x_test  = np.load(os.path.join(data_dir, 'x_test.npy'))[..., :6]\n",
        "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy')) - 1\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# Dictionary containing the labels and the associated activities\n",
        "label_to_activity = {0: 'walking horizontally', 1: 'walking upstairs', 2: 'walking downstairs',\n",
        "                     3: 'sitting', 4: 'standing', 5: 'laying'}\n",
        "\n",
        "# Function to change integer labels to one-hot labels\n",
        "def integer_to_onehot(data_integer):\n",
        "    data_onehot = np.zeros(shape=(data_integer.shape[0], data_integer.max()+1))\n",
        "    for row in range(data_integer.shape[0]):\n",
        "        integer = int(data_integer[row])\n",
        "        data_onehot[row, integer] = 1\n",
        "    return data_onehot\n",
        "\n",
        "# Load the data\n",
        "(x_train, y_train), (x_test, y_test) = load_HAR_data()\n",
        "y_train_oh = integer_to_onehot(y_train)\n",
        "y_test_oh = integer_to_onehot(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUybzk8lKaqa"
      },
      "source": [
        "# Inspect some of the data by making plots\n",
        "\n",
        "def make_plots(num_examples_per_category):\n",
        "    for label in range(6):\n",
        "        x_label = x_train[y_train[:, 0] == label]\n",
        "        for i in range(num_examples_per_category):\n",
        "            fig, ax = plt.subplots(figsize=(10, 1))\n",
        "            ax.imshow(x_label[100*i].T, cmap='Greys', vmin=-1, vmax=1)\n",
        "            ax.axis('off')\n",
        "            if i == 0:\n",
        "                ax.set_title(label_to_activity[label])\n",
        "            plt.show()\n",
        "        \n",
        "make_plots(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxhEIraGKaqb"
      },
      "source": [
        "#### 1D deterministic convolutional neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CV7Sxy6Kaqc"
      },
      "source": [
        "# Create standard deterministic model with:\n",
        "# - Conv1D\n",
        "# - MaxPooling\n",
        "# - Flatten\n",
        "# - Dense with Softmax\n",
        "\n",
        "model = Sequential([\n",
        "    Conv1D(input_shape=(128, 6), filters=8, kernel_size=16, activation='relu'),\n",
        "    MaxPooling1D(pool_size=16),\n",
        "    Flatten(),\n",
        "    Dense(units=6, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsUT-G8YKaqd"
      },
      "source": [
        "#### Probabilistic 1D convolutional neural network, with both weight and output uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euajafSYKaqe"
      },
      "source": [
        "# Create probablistic model with the following layers:\n",
        "#  - Conv1D\n",
        "#  - MaxPooling\n",
        "#  - Flatten\n",
        "#  - Dense\n",
        "#  - OneHotCategorical\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbS5vk0_Kaqg"
      },
      "source": [
        "# Replace analytical Kullback-Leibler divergence with approximated one\n",
        "\n",
        "def kl_approx(q, p, q_tensor):\n",
        "    return tf.reduce_mean(q.log_prob(q_tensor) - p.log_prob(q_tensor))\n",
        "\n",
        "divergence_fn = lambda q, p, q_tensor : kl_approx(q, p, q_tensor) / x_train.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkwLmGHaKaqh"
      },
      "source": [
        "# Compile the model using the negative loglikelihood\n",
        "\n",
        "def nll(y_true, y_pred):\n",
        "    return -y_pred.log_prob(y_true)\n",
        "\n",
        "model.compile(loss=nll,\n",
        "              optimizer=RMSprop(learning_rate=0.005),\n",
        "              metrics=['accuracy'],\n",
        "              experimental_run_tf_function=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibkhBetoKaqj"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(x_train, y_train_oh, epochs=20, verbose=False)\n",
        "model.evaluate(x_train, y_train_oh)\n",
        "model.evaluate(x_test, y_test_oh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtBXmPJIKaql"
      },
      "source": [
        "#### Inspect model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBnw9p_tKaqm"
      },
      "source": [
        "# Define function to analyse model predictions versus true labels\n",
        "\n",
        "def analyse_model_predictions(image_num):\n",
        "\n",
        "    # Show the accelerometer data\n",
        "    print('------------------------------')\n",
        "    print('Accelerometer data:')\n",
        "    fig, ax = plt.subplots(figsize=(10, 1))\n",
        "    ax.imshow(x_test[image_num].T, cmap='Greys', vmin=-1, vmax=1)\n",
        "    ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Print the true activity\n",
        "    print('------------------------------')\n",
        "    print('True activity:', label_to_activity[y_test[image_num, 0]])\n",
        "    print('')\n",
        "\n",
        "    # Print the probabilities the model assigns\n",
        "    print('------------------------------')\n",
        "    print('Model estimated probabilities:')\n",
        "    # Create ensemble of predicted probabilities\n",
        "    predicted_probabilities = np.empty(shape=(200, 6))\n",
        "    for i in range(200):\n",
        "        predicted_probabilities[i] = model(x_test[image_num][np.newaxis, ...]).mean().numpy()[0]\n",
        "    pct_2p5 = np.array([np.percentile(predicted_probabilities[:, i], 2.5) for i in range(6)])\n",
        "    pct_97p5 = np.array([np.percentile(predicted_probabilities[:, i], 97.5) for i in range(6)])\n",
        "    # Make the plots\n",
        "    fig, ax = plt.subplots(figsize=(9, 3))\n",
        "    bar = ax.bar(np.arange(6), pct_97p5, color='red')\n",
        "    bar[y_test[image_num, 0]].set_color('green')\n",
        "    bar = ax.bar(np.arange(6), pct_2p5-0.02, color='white', linewidth=1, edgecolor='white')\n",
        "    ax.set_xticklabels([''] + [activity for activity in label_to_activity.values()],\n",
        "                       rotation=45, horizontalalignment='right')\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.set_ylabel('Probability')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibhqPDcJKaqo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwg676VGKaqr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL9SAg6bKaqu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}